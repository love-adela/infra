[[34m2021-03-26 11:40:14,208[0m] {[34mscheduler_job.py:[0m1247} INFO[0m - Starting the scheduler[0m
[[34m2021-03-26 11:40:14,209[0m] {[34mscheduler_job.py:[0m1252} INFO[0m - Processing each file at most -1 times[0m
[[34m2021-03-26 11:40:14,218[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 67700[0m
[[34m2021-03-26 11:40:14,219[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-26 11:40:14,678[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2021-03-26 11:40:14,681] {dag_processing.py:515} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.
[[34m2021-03-26 11:45:14,612[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-26 11:50:15,312[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-26 11:51:48,094[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 4 tasks up for execution:
	<TaskInstance: example_bash_operator.runme_0 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this 2021-03-24 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-26 11:51:48,096[0m] {[34mscheduler_job.py:[0m970} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 4 task instances ready to be queued[0m
[[34m2021-03-26 11:51:48,096[0m] {[34mscheduler_job.py:[0m998} INFO[0m - DAG example_bash_operator has 0/16 running and queued tasks[0m
[[34m2021-03-26 11:51:48,097[0m] {[34mscheduler_job.py:[0m998} INFO[0m - DAG example_bash_operator has 1/16 running and queued tasks[0m
[[34m2021-03-26 11:51:48,097[0m] {[34mscheduler_job.py:[0m998} INFO[0m - DAG example_bash_operator has 2/16 running and queued tasks[0m
[[34m2021-03-26 11:51:48,098[0m] {[34mscheduler_job.py:[0m998} INFO[0m - DAG example_bash_operator has 3/16 running and queued tasks[0m
[[34m2021-03-26 11:51:48,098[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: example_bash_operator.runme_0 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 2021-03-24 00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this 2021-03-24 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-26 11:51:48,100[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', execution_date=datetime.datetime(2021, 3, 24, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-26 11:51:48,100[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:48,101[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', execution_date=datetime.datetime(2021, 3, 24, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-26 11:51:48,101[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:48,102[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', execution_date=datetime.datetime(2021, 3, 24, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-03-26 11:51:48,102[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:48,103[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', execution_date=datetime.datetime(2021, 3, 24, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-03-26 11:51:48,103[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:48,105[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:48,623[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py[0m
[[34m2021-03-26 11:51:48,647[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-26 11:51:48,647[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: example_bash_operator.runme_0 2021-03-24T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2021-03-26 11:51:49,945[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:50,531[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py[0m
[[34m2021-03-26 11:51:50,559[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-26 11:51:50,559[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: example_bash_operator.runme_1 2021-03-24T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2021-03-26 11:51:51,872[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:52,578[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py[0m
[[34m2021-03-26 11:51:52,609[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-26 11:51:52,610[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: example_bash_operator.runme_2 2021-03-24T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2021-03-26 11:51:53,936[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2021-03-24T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py'][0m
[[34m2021-03-26 11:51:54,631[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/example_dags/example_bash_operator.py[0m
[[34m2021-03-26 11:51:54,667[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-03-26 11:51:54,668[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: example_bash_operator.also_run_this 2021-03-24T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2021-03-26 11:51:54,960[0m] {[34mscheduler_job.py:[0m1199} INFO[0m - Executor reports execution of example_bash_operator.runme_0 execution_date=2021-03-24 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-26 11:51:54,961[0m] {[34mscheduler_job.py:[0m1199} INFO[0m - Executor reports execution of example_bash_operator.runme_1 execution_date=2021-03-24 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-26 11:51:54,961[0m] {[34mscheduler_job.py:[0m1199} INFO[0m - Executor reports execution of example_bash_operator.runme_2 execution_date=2021-03-24 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-26 11:51:54,962[0m] {[34mscheduler_job.py:[0m1199} INFO[0m - Executor reports execution of example_bash_operator.also_run_this execution_date=2021-03-24 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-03-26 11:55:15,983[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-03-26 11:55:15,988[0m] {[34mscheduler_job.py:[0m1898} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: example_bash_operator.run_after_loop 2021-03-24 00:00:00+00:00 [scheduled]>[0m
[[34m2021-03-26 11:57:12,164[0m] {[34mscheduler_job.py:[0m1298} ERROR[0m - Exception when executing SchedulerJob._run_scheduler_loop[0m
Traceback (most recent call last):
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 364, in connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 508, in connect
sqlite3.OperationalError: unable to open database file

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1280, in _execute
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1382, in _run_scheduler_loop
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1478, in _do_scheduling
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1581, in _create_dagruns_for_dags
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/tenacity/__init__.py", line 390, in __iter__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/tenacity/__init__.py", line 368, in iter
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/tenacity/__init__.py", line 186, in reraise
  File "/usr/local/Cellar/python@3.9/3.9.2_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 433, in result
    return self.__get_result()
  File "/usr/local/Cellar/python@3.9/3.9.2_2/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1590, in _create_dagruns_for_dags
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3373, in all
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3535, in __iter__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3556, in _execute_and_instances
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3571, in _get_bind_args
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3550, in _connection_from_session
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1142, in connection
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1150, in _connection_for_bind
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 433, in _connection_for_bind
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2302, in _contextual_connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2339, in _wrap_pool_connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1583, in _handle_dbapi_exception_noconnection
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 364, in connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
  File "/Users/adela/workspace/infra/airflow/myvenv/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 508, in connect
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file
(Background on this error at: http://sqlalche.me/e/13/e3q8)[0m
[[34m2021-03-26 11:57:13,180[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 67700[0m
[[34m2021-03-26 11:57:13,398[0m] {[34mprocess_utils.py:[0m66} INFO[0m - Process psutil.Process(pid=67700, status='terminated', exitcode=0, started='11:40:14') (67700) terminated with exit code 0[0m
[[34m2021-03-26 11:57:13,399[0m] {[34mscheduler_job.py:[0m1301} INFO[0m - Exited execute loop[0m
